<html>
<head>
    <link rel="stylesheet" href="css/academicons.min.css"/>
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Jacob Chalk</title>

    <!-- Bootstrap core CSS -->
    <link href="vendor/bootstrap/css/bootstrap.min.css" rel="stylesheet">

    <!-- Custom fonts for this template -->
    <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:100,200,300,400,500,600,700,800,900" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Open+Sans:300,300i,400,400i,600,600i,700,700i,800,800i" rel="stylesheet">
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
    <link href="vendor/devicons/css/devicons.min.css" rel="stylesheet">
    <link href="vendor/simple-line-icons/css/simple-line-icons.css" rel="stylesheet">

    <!-- Custom styles for this template -->
    <link href="css/website.min.css" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="css/style.css">
    <link rel="shortcut icon" type="image/ico" href="favicon.ico" />
    <link href="vendor/font-awesome/css/font-awesome.min.css" rel="stylesheet">
</head>
<body>
    <header class="site-header"><a class="site-title" href="/">Jacob Chalk - Research Associate</a></header>
    <div id="mainBar">
        <div id="textBar">
            
            <img class="profile-img" src="img/profile.jpg" alt="Update" style="width:180px;float:right;margin-left: 5% auto;border-radius:50%" vspace="20" hspace="25">
            
            <h2>About</h2>
            
            <p>I am a Research Associate at the University of Bristol and a member of the <a href="https://uob-mavi.github.io/">MaVi</a> research group. My current
            research is on 4D video understanding, aiming to develop systems that can perceive and reason about dynamic 3D scenes over time.</p>
            
            <p>Previously, as a PhD researcher of Computer Vision at the University of Bristol, supervised by <a href="https://dimadamen.github.io">Prof. Dima Damen</a>,
            my research focus was on leveraging multimodal data for egocentric video understanding. This included topics such as audio-visual deep learning, 
            action recognition/detection, predicting object-interactions using eye-gaze and 3D annotations, and long-term 3D multi-object tracking. During this time, 
            I was also a PhD intern with the <a href="https://europe.naverlabs.com/research/lifelong-learning-for-visual-representation">Visual Representation Learning team</a> 
            at <a href="https://europe.naverlabs.com">Naver Labs Europe</a>.</p>
            
            <p>Prior to my PhD, I earned a First Class Honours MEng in Computer Science from the University of Bristol, 
            where my dissertation on "Video GANs for Human-Object Interactions" was highly graded. Alongside research, 
            I've gained teaching experience across multiple undergraduate modules, contributing to both coursework 
            design and lab-based support.</p>
            
            <p>I've worked across a range of projects involving large-scale multimodal datasets and model development, 
            contributing to research outputs such as HD-EPIC, EPIC-Sounds, TIM, and OSNOM (see Research). My experience 
            spans dataset construction, multimodal model design, and open-source codebase development.</p>
            
            <p>My technical strengths lie in deep learning, computer vision, and multimodal modelling, with extensive
            experience in Python (PyTorch) and capabilities with C++ and Javascript.</p>
            
            <ul class="list-inline list-social-icons mb-0">
                <li class="list-inline-item">
                    <a href="pdfs/cv.pdf">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-id-card fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>
            
                <li class="list-inline-item">
                    <a href="https://scholar.google.com/citations?user=FPugmicAAAAJ&hl=en">
                        <span class="fa-stack fa-lg">
                            <i class="ai ai-google-scholar-square ai-2x"></i>
                        </span>
                    </a>
                </li>
            
                <li class="list-inline-item">
                    <a href="https://github.com/jacobchalk">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-github fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>
            
                <li class="list-inline-item">
                    <a href="https://x.com/jacobchalkie">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-twitter fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>
            
                <li class="list-inline-item">
                    <a href="https://www.linkedin.com/in/jacob-chalk">
                        <span class="fa-stack fa-lg">
                            <i class="fa fa-square fa-stack-2x"></i>
                            <i class="fa fa-linkedin fa-stack-1x fa-inverse"></i>
                        </span>
                    </a>
                </li>
            </ul>
            
            <h4> Email: jacob.chalk@bristol.ac.uk </h4>
            
            <hr>
            
            <h3>News</h3>

            <!--news_section-->
            <ul class="no-bullets">
                <li> <span class="prim-colour">January 2026</span> - <b>New Role:</b> Started as a Research Associate at the <a href="https://uob-mavi.github.io/people/">University of Bristol</a>!</li>
                <li> <span class="prim-colour">December 2025</span> - <b>New Paper:</b> <a href="https://masashi-hatano.github.io/prime-and-reach/">Prime-and-reach</a> paper published to arXiv!</li>
                <li> <span class="prim-colour">February 2025</span> - <b>New Role:</b> Started as a PhD Intern at <a href="https://europe.naverlabs.com">Naver Labs Europe</a>!</li>
                <li> <span class="prim-colour">February 2025</span> - <b>New Dataset:</b> <a href="https://hd-epic.github.io">HD-EPIC</a> has been publically released and accepted to CVPR 2025!</li>
                <li> <span class="prim-colour">January 2025</span> - <b>Code Released:</b> <a href="https://dimadamen.github.io/OSNOM">OSNOM</a> code and camera-ready paper have been released!</li>
                <li> <span class="prim-colour">November 2024</span> - <b>New Paper:</b> <a href="https://dimadamen.github.io/OSNOM">OSNOM</a> paper published to arXiv and accepted to 3DV 2025!</li>
                <li> <span class="prim-colour">September 2024</span> - <b>Journal Paper:</b> <a href="https://epic-kitchens.github.io/epic-sounds">EPIC-Sounds</a> Journal Extended version is now available on arXiv!</li>
                <li> <span class="prim-colour">April 2024</span> - <b>New Paper:</b> <a href="https://jacobchalk.github.io/TIM-Project">TIM</a> paper and code have been released and accepted to CVPR 2024!</li>
                <li> <span class="prim-colour">February 2023</span> - <b>Paper Accepted:</b> <a href="https://epic-kitchens.github.io/epic-sounds">EPIC-Sounds</a> has been accepted to ICASSP 2023!</li>
                <li> <span class="prim-colour">January 2023</span> - <b>New Dataset:</b> <a href="https://epic-kitchens.github.io/epic-sounds">EPIC-Sounds</a> has been publically released!</li>
                <li> <span class="prim-colour">September 2021</span> - <b>New Role:</b> Started as a PhD Researcher with <a href="https://dimadamen.github.io">Prof. Dima Damen</a>!</li>
            </ul>
            <!--end_news-->

            <hr>

            <h3>Research</h3>

            <p><em>Current list of all research projects:</em><p>

            <!--research_section-->
            <table class="researchtable">
                <tbody>
                    <tr>
                        <td class=img>
                            <img src="img/papers/prime-and-reach.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Prime and Reach: Synthesising Body Motion for Gaze-Primed Object Reach</span>
                            <br>
                            Masashi Hatano*, Saptarshi Sinha*, <span class="author">Jacob Chalk</span>, Wei-Hong Li, Hideo Saito, Dima Damen
                            <br>
                            *: Equal Contribution
                            <br>
                            <em> arXiv preprint arXiv:2512.16456, 2025</em>
                            <br>
                            <strong>
                                <a href="https://masashi-hatano.github.io/prime-and-reach/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2512.16456">[arXiv]</a>
                                <a href="https://github.com/masashi-hatano/prime-and-reach"> [Code & Data]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/papers/hd-epic.jpg">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">HD-EPIC: A Highly-Detailed Egocentric Video Dataset</span>
                            <br>
                            Toby Perrett*, Ahmad Darkhalil*, Saptarshi Sinha*, Omar Emara*, Sam Pollard*, Kranti Parida*, Kaiting Liu*, Prajwal Gatti*, Siddhant Bansal*, Kevin Flanagan*, <span class="author">Jacob Chalk*</span>, Zhifan Zhu*, Rhodri Guerrier*, Fahd Abdelazim*, Bin Zhu, Davide Moltisanti, Michael Wray, Hazel Doughty, Dima Damen
                            <br>
                            *: Equal Contribution
                            <br>
                            <em>Conference on Computer Vision and Pattern Recognition (CVPR), 2025</em>
                            <br>
                            <strong>
                                <a href="https://hd-epic.github.io/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2502.04144">[arXiv]</a>
                                <a href="https://github.com/hd-epic/hd-epic-annotations"> [Code & Data]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/papers/osnom.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">Spatial Cognition from Egocentric Video: Out of Sight, Not Out of Mind</span>
                            <br>
                            Chiara Plizzari, Shubham Goel, Toby Perrett, <span class="author">Jacob Chalk</span>, Angjoo Kanazawa, Dima Damen
                            <br>
                            <em>International Conference on 3D Vision (3DV), 2025</em>
                            <br>
                            <strong>
                                <a href="https://dimadamen.github.io/OSNOM/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2404.05072">[arXiv]</a>
                                <a href="https://github.com/Chiaraplizz/OSNOM"> [Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/papers/tim.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">TIM: A Time Interval Machine for Audio-Visual Action Recognition</span>
                            <br>
                            <span class="author">Jacob Chalk*</span>, Jaesung Huh*, Evangelos Kazakos, Andrew Zisserman, Dima Damen
                            <br>
                            *: Equal Contribution
                            <br>
                            <em>Conference on Computer Vision and Pattern Recognition (CVPR), 2024</em>
                            <br>
                            <strong>
                                <a href="https://jacobchalk.github.io/TIM-Project/">[Webpage]</a>
                                <a href="https://arxiv.org/abs/2404.05559">[arXiv]</a>
                                <a href="https://github.com/JacobChalk/TIM">[Code]</a>
                            </strong>
                        </td>
                    </tr>
                    <tr>
                        <td class=img>
                            <img src="img/papers/epic-sounds.png">
                        </td>
                        <td valign="top">
                            <span class="prim-colour">EPIC-Sounds: A Large-scale Dataset of Actions That Sound</span>
                            <br>
                            Jaesung Huh*, <span class="author">Jacob Chalk*</span>, Evangelos Kazakos, Dima Damen, Andrew Zisserman
                            <br>
                            *: Equal Contribution
                            <br>
                            <em>IEEE International Conference on Acoustics, Speech, and Signal Processing (ICASSP), 2023</em>
                            <br>
                            <strong>
                                <a href="https://epic-kitchens.github.io/epic-sounds/">[Webpage]</a>
                                <a href="https://arxiv.org/2302.00646">[arXiv]</a>
                                <a href="https://github.com/epic-kitchens/epic-sounds-annotations">[Code & Data]</a>
                            </strong>
                        </td>
                    </tr>
                </tbody>
            </table>
            <!--end_research-->

            <hr>

            <h3>Teaching</h3>

            <ul class="no-bullets">
                <li>Teaching Assistant</li>
                <li>
                    <ul class="no-bullets">
                        <li><span class="prim-colour">Applied Deep Learning</span> - 21/22, 22/23, 23/24, 24/25. <a href="https://comsm0045-applied-deep-learning.github.io/">Webpage</a>.
                    </ul>
                    <ul class="no-bullets">
                        <li><span class="prim-colour">Image Processing and Computer Vision</span> - 23/24. <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=25%2F26&unitCode=COMS30030">Webpage</a>.
                    </ul>
                    <ul class="no-bullets">
                        <li><span class="prim-colour">Computer Graphics</span> - 20/21, 21/22, 23/24. <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=25%2F26&unitCode=COMS30020">Webpage</a>.
                    </ul>
                    <ul class="no-bullets">
                        <li><span class="prim-colour">Team Project</span> - 20/21 <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=25%2F26&unitCode=COMS30043">Webpage</a>.
                    </ul>
                    <ul class="no-bullets">
                        <li><span class="prim-colour">Software Engineering Product</span> - 19/20, 20/21. <a href="https://www.bris.ac.uk/unit-programme-catalogue/UnitDetails.jsa?ayrCode=25%2F26&unitCode=COMS20006">Webpage</a>.
                    </ul>
                </li>
            </ul>

            <hr>

            <h3>Miscellaneous</h3>

            <h5>Presentations</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">EPIC-KITCHENS Challenges</span> - First Joint Egocentric Vision Workshop (CVPR 2024)</li>
                <li><span class="prim-colour">EPIC-Sounds Oral Presentation</span> - ICASSP 2023</li>
            </ul>
            
            <h5>Conference Reviewer</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">Conference on Neural Information Processing Systems (NeurIPS)</span> - 2025</li>
                <li><span class="prim-colour">International Conference on Computer Vision (ICCV)</span> - 2025</li>
                <li><span class="prim-colour">Conference on Computer Vision and Pattern Recognition (CVPR)</span> - 2025</li>
                <li><span class="prim-colour">European Conference on Computer Vision (ECCV)</span> - 2024</li>
            </ul>

            <h5>Journal Reviewer</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">IEEE Open Journal of Signal Processing (OJSP) </span> - 2025</li>
                <li><span class="prim-colour">International Journal of Computer Vision (IJCV) </span> - 2025</li>
                <li><span class="prim-colour">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</span> - 2024</li>
            </ul>

            <h5>Honours and Awwards</h5>

            <ul class="no-bullets">
                <li><span class="prim-colour">Outstanding Reviewer</span>
                    <ul>
                        <li>International Conference on Computer Vision (ICCV 2025)</li>
                        <li>Conference on Computer Vision and Pattern Recognition (CVPR 2025)</li>
                    </ul>
                <li><span class="prim-colour">EgoVis 2022/23 Distinguished Paper Awards</span> - First Joint Egocentric Vision Workshop (CVPR 2024)</li>
                <li><span class="prim-colour">EPIC-KITCHENS Challenges Winner</span> - First Joint Egocentric Vision Workshop (CVPR 2024)</li>
                <li>
                    <ul class="no-bullets">
                        <li>Audio-Based Interaction Recognition (2nd)</li>
                        <li>Audio-Based Interaction Detection (2nd)</li>
                        <li>Action Detection (3rd)</li>
                    </ul>
                </li>
                <li><span class="prim-colour">Top 5 Third Year MEng Computer Science/Computer Science with Maths</span> - Awarded by Netcraft (University of Bristol 2020)</li>
            </ul>

            <hr>
        </div>

    </div>
    <div id="modal01" class="w3-modal" onclick="this.style.display='none'">
      <div class="w3-modal-content w3-animate-zoom">
        <img id="img01" style="width:75%;display:block;margin-left:auto;margin-right:auto;">
      </div>
    </div>
</body>
